import json
import os
import re
import sentencepiece as spm
from collections import Counter

def merge_json_files(input_folder, output_file):
    all_data = []
    
    # ëª¨ë“  JSON íŒŒì¼ ì²˜ë¦¬
    for filename in os.listdir(input_folder):
        if filename.endswith('.json'):
            file_path = os.path.join(input_folder, filename)
            print(f"Processing: {filename}")
            
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                
                # 'data' í‚¤ ì•ˆì˜ ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ
                if 'data' in data:
                    all_data.extend(data['data'])
                    print(f"  â†’ {len(data['data'])}ê°œ ë¬¸ì¥ ìŒ ì¶”ê°€ë¨")
    
    # í•©ì¹œ ë°ì´í„° ì €ì¥
    merged_data = {"data": all_data}
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
    output_dir = os.path.dirname(output_file)
    if output_dir and not os.path.exists(output_dir):
        os.makedirs(output_dir)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(merged_data, f, ensure_ascii=False, indent=2)
    
    print(f"ì´ {len(all_data)}ê°œ ë¬¸ì¥ ìŒì´ í•©ì³ì¡ŒìŠµë‹ˆë‹¤!")
    return len(all_data)



def is_korean(text):
    return bool(re.search(r'[ê°€-í£]', text))

def is_english(text):
    return bool(re.search(r'[a-zA-Z]', text))

def has_spacing_issue(text):
    if re.search(r'\s{5,}', text):
        return True
    if len(text.split()) <= 1 and len(text) > 20:
        return True
    return False

def contains_bad_tags(text):
    tag_patterns = [
        r'<[^>]+>',
        r'&[a-z]+;',
        r'\[[^\]]{10,}\]',
        r'\{[^\}]{10,}\}',
        r'\([^)]{30,}\)',
    ]
    return any(re.search(p, text) for p in tag_patterns)

def clean_data(input_file, output_file):
    with open(input_file, 'r', encoding='utf-8') as f:
        data = json.load(f)

    cleaned_data = []
    seen_pairs = set()
    min_len = 3
    MAX_LENGTH = 300  # ì™„í™”ëœ ìµœëŒ€ ê¸¸ì´

    drop_counter = Counter()

    for item in data['data']:
        korean = item['source_sentence'].strip()
        english = item['target_sentence'].strip()

        pair = (korean, english)
        if pair in seen_pairs:
            drop_counter['ì¤‘ë³µ'] += 1
            continue

        if len(korean) < min_len or len(english) < min_len:
            drop_counter['ì§§ì€ ë¬¸ì¥'] += 1
            continue

        if len(korean) > MAX_LENGTH or len(english) > MAX_LENGTH:
            drop_counter['ê¸´ ë¬¸ì¥'] += 1
            continue

        if not (is_korean(korean) or is_english(korean)):
            drop_counter['í•œêµ­ì–´/ì˜ì–´ ì—†ëŠ” ë¬¸ì¥ (ì†ŒìŠ¤)'] += 1
            continue

        if not (is_korean(english) or is_english(english)):
            drop_counter['í•œêµ­ì–´/ì˜ì–´ ì—†ëŠ” ë¬¸ì¥ (íƒ€ê²Ÿ)'] += 1
            continue

        if has_spacing_issue(korean) or has_spacing_issue(english):
            drop_counter['ë„ì–´ì“°ê¸° ë¬¸ì œ'] += 1
            continue

        if contains_bad_tags(korean) or contains_bad_tags(english):
            drop_counter['íƒœê·¸ ë¬¸ì œ'] += 1
            continue

        cleaned_data.append({
            'source_sentence': korean,
            'target_sentence': english,
            'domain': item.get('domain', 'unknown')
        })
        seen_pairs.add(pair)

    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump({'data': cleaned_data}, f, ensure_ascii=False, indent=2)

    total = len(data['data'])
    kept = len(cleaned_data)
    dropped = total - kept

    print(f"ì •ì œ ì „: {total}ê°œ â†’ ì •ì œ í›„: {kept}ê°œ (ë“œë {dropped}ê°œ)")
    print("ë“œë ì‚¬ìœ ë³„ í†µê³„:")
    for reason, count in drop_counter.most_common():
        print(f"  {reason}: {count}ê°œ")

# opennmt formatìœ¼ë¡œ ë³€ê²½
def convert_to_opennmt_format(json_file, ko_file, en_file):
    with open(json_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    with open(ko_file, 'w', encoding='utf-8') as ko_f, \
         open(en_file, 'w', encoding='utf-8') as en_f:
        
        for item in data['data']:
            # AI Hub êµ¬ì¡°ì—ì„œ ë¬¸ì¥ ì¶”ì¶œ
            korean = item['source_sentence']
            english = item['target_sentence']
            
            ko_f.write(korean + '\n')
            en_f.write(english + '\n')
    
    print(f"{len(data['data'])}ê°œ ë¬¸ì¥ ìŒ ë³€í™˜ ì™„ë£Œ!")
    
# í•œêµ­ì–´ì™€ ì˜ì–´ í…ìŠ¤íŠ¸ í•©ì¹˜ê¸° (í† í¬ë‚˜ì´ì € í•™ìŠµìš©)
def prepare_tokenizer_data():
    with open('tokenizer_corpus.txt', 'w', encoding='utf-8') as f:
        # í•œêµ­ì–´ ë°ì´í„° ì¶”ê°€
        with open('train.ko', 'r', encoding='utf-8') as ko_f:
            for line in ko_f:
                f.write(line)
        
        # ì˜ì–´ ë°ì´í„° ì¶”ê°€
        with open('train.en', 'r', encoding='utf-8') as en_f:
            for line in en_f:
                f.write(line)
                


# SentencePieceë¡œ í† í¬ë‚˜ì´ì§• (OpenNMTìš©)
def tokenize_data():
    sp = spm.SentencePieceProcessor()
    sp.load('sentencepiece.model')
    
    # Training ë°ì´í„° í† í¬ë‚˜ì´ì§•
    tokenize_file(sp, 'train.ko', 'train.tok.ko')
    tokenize_file(sp, 'train.en', 'train.tok.en')
    
    # Validation ë°ì´í„° í† í¬ë‚˜ì´ì§•
    tokenize_file(sp, 'valid.ko', 'valid.tok.ko')
    tokenize_file(sp, 'valid.en', 'valid.tok.en')



def tokenize_file(sp, input_file, output_file):
    """ê°œë³„ íŒŒì¼ í† í¬ë‚˜ì´ì§•"""
    with open(input_file, 'r', encoding='utf-8') as f_in, \
         open(output_file, 'w', encoding='utf-8') as f_out:
        for line in f_in:
            tokens = sp.encode_as_pieces(line.strip())
            f_out.write(' '.join(tokens) + '\n')



def merge_data_json(file1, file2, output_file):
    """
    {"data": [...]} í˜•íƒœì˜ JSON íŒŒì¼ë“¤ì„ í•©ì¹˜ëŠ” í•¨ìˆ˜
    """
    # ì²« ë²ˆì§¸ íŒŒì¼ ì½ê¸°
    with open(file1, 'r', encoding='utf-8') as f:
        data1 = json.load(f)
    
    # ë‘ ë²ˆì§¸ íŒŒì¼ ì½ê¸°
    with open(file2, 'r', encoding='utf-8') as f:
        data2 = json.load(f)
    
    # "data" í‚¤ ì•ˆì˜ ë¦¬ìŠ¤íŠ¸ë“¤ í•©ì¹˜ê¸°
    merged_data = {
        "data": data1["data"] + data2["data"]
    }
    
    # í•©ì³ì§„ ë°ì´í„° ì €ì¥
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(merged_data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… íŒŒì¼ í•©ì¹˜ê¸° ì™„ë£Œ!")
    print(f"ğŸ“Š íŒŒì¼1 ë°ì´í„°: {len(data1['data'])}ê°œ")
    print(f"ğŸ“Š íŒŒì¼2 ë°ì´í„°: {len(data2['data'])}ê°œ") 
    print(f"ğŸ“Š í•©ì³ì§„ ë°ì´í„°: {len(merged_data['data'])}ê°œ")
    print(f"ğŸ’¾ ì €ì¥ëœ íŒŒì¼: {output_file}")
    
    return merged_data

