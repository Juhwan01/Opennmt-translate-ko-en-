# 데이터 및 어휘 설정
save_data: data/demo
# 이거 때문에 새벽 5시까지 시간 버림...
src_vocab: spm.onmt.vocab
tgt_vocab: spm.onmt.vocab

data:
    corpus_1:
        path_src: train.ko
        path_tgt: train.en
    valid:
        path_src: valid.ko
        path_tgt: valid.en

src_vocab_size: 30000
tgt_vocab_size: 30000

# 이렇게 해야 추론할때도 학습할때도 토큰화 안해서 넣어도 됨 
src_subword_model: sentencepiece.model
tgt_subword_model: sentencepiece.model

# 추론/검증 설정 (validation 시 사용됨)
beam_size: 5

# Seq2Seq:
encoder_type: rnn
decoder_type: rnn
rnn_type: LSTM

#Transformer:
# encoder_type: transformer
# decoder_type: transformer

# 학습 설정
batch_size: 32
train_steps: 20000         
valid_steps: 1000          
save_checkpoint_steps: 1000 

# 체크포인트 관리 (자동 최적 모델 저장)
keep_checkpoint: 10                     # 최근 5개 + best 모델 유지
save_model_steps: 1000                 # 저장 주기 설정

# 조기 종료 설정 (과적합 방지)
early_stopping: 5
early_stopping_criteria: [ppl]

save_best_model: true

# 문장길이제한
src_seq_length: 300
tgt_seq_length: 300

# 옵티마이저
optim: adam
learning_rate: 0.001
learning_rate_decay: 0.8               # 성능 정체 시 학습률 감소
start_decay_steps: 6000               # 6000스텝부터 감소 시작
decay_steps: 1000                      # 1000스텝마다 감소 체크

# 저장 경로
save_model: models/model
log_file: logs/train.log

# 로그 설정
report_every: 100
tensorboard: true
tensorboard_log_dir: logs/tensorboard  

# GPU 설정
world_size: 1
gpu_ranks: [0]