import sentencepiece as smp
import torch
from onmt.translate.translator import build_translator
import argparse
from onmt.opts import translate_opts


class KoreanEnglishTranslator:
    def __init__(self, model_path, tokenizer_path):
        # SentencePiece í† í¬ë‚˜ì´ì € ë¡œë“œ
        self.sp = smp.SentencePieceProcessor()
        self.sp.load(tokenizer_path)
        
        # OpenNMT ëª¨ë¸ ë¡œë“œ
        self.translator = self._load_translator(model_path)
    
    def _load_translator(self, model_path):
        parser = argparse.ArgumentParser()
        translate_opts(parser)
        
        args = parser.parse_args([
            '--model', model_path,
            '--beam_size', '5',
            '--batch_size', '1',
            '--replace_unk',
            '--src', 'dummy.txt'
        ] + (['--gpu', '0'] if torch.cuda.is_available() else []))
        
        translator = build_translator(args, report_score=False)
        return translator
    
    def translate(self, korean_text):
        import os
        os.environ["CUDA_LAUNCH_BLOCKING"] = "1"  # ì—ëŸ¬ ì •í™•íˆ í™•ì¸

        korean_text = korean_text.strip()
        if not korean_text:
            return ""

        # SentencePiece í† í¬ë‚˜ì´ì§•
        tokens = self.sp.encode_as_pieces(korean_text)
        
        # í† í° ID ì¶”ì¶œ + ê²€ì‚¬
        token_ids = []
        for token in tokens:
            token_id = self.sp.piece_to_id(token)
            if token_id < 0:
                print(f"[ê²½ê³ ] vocabì— ì—†ëŠ” token: {token}, ID: {token_id}")
                token_id = self.sp.unk_id()
            token_ids.append(token_id)

        print("ğŸ‘‰ token_ids:", token_ids)

        # vocab size í™•ì¸ - ì£¼ì„ í•´ì œ ë° ê°œì„ 
        try:
            vocab_size = self.translator.model.encoder.embeddings.make_embedding.emb_luts[0].num_embeddings
        except AttributeError:
            try:
                vocab_size = self.translator.model.encoder.embeddings.emb_luts[0].num_embeddings
            except AttributeError:
                try:
                    vocab_size = self.translator.model.src_embeddings.num_embeddings
                except AttributeError:
                    vocab_size = self.sp.get_piece_size()
                    print(f"[ê²½ê³ ] ëª¨ë¸ vocab size ìë™ ê°ì§€ ì‹¤íŒ¨, SentencePiece vocab size ì‚¬ìš©: {vocab_size}")
        
        print(f"ğŸ“Š ëª¨ë¸ vocab size: {vocab_size}")
        print(f"ğŸ“Š SentencePiece vocab size: {self.sp.get_piece_size()}")
        
        # vocab size ë²”ìœ„ ì²´í¬
        validated_token_ids = []
        for idx in token_ids:
            if idx >= vocab_size:
                print(f"[ê²½ê³ ] token_id {idx}ëŠ” vocab_size {vocab_size} ì´ˆê³¼ -> UNK({self.sp.unk_id()})ë¡œ ë³€ê²½")
                validated_token_ids.append(self.sp.unk_id())
            else:
                validated_token_ids.append(idx)
        
        print("âœ… ê²€ì¦ëœ token_ids:", validated_token_ids)

        # í…ì„œ ìƒì„± - ì°¨ì› ìˆ˜ì •: [seq_len, batch_size] í˜•íƒœë¡œ ë³€ê²½
        src_tensor = torch.tensor(validated_token_ids, dtype=torch.long).unsqueeze(1)  # [seq_len, 1]
        src_len = torch.tensor([len(validated_token_ids)], dtype=torch.long)

        print("âœ… í…ì„œ ìƒì„± ì™„ë£Œ:", src_tensor.shape, src_tensor.dtype)
        print("âœ… ê¸¸ì´ í…ì„œ:", src_len.shape, src_len.dtype)

        # GPU ì „ì†¡ ì „ shape/type ê²€ì‚¬
        if torch.any(src_tensor < 0):
            raise ValueError("[ì—ëŸ¬] src_tensorì— ìŒìˆ˜ ID í¬í•¨")
        if torch.any(src_tensor >= vocab_size):
            raise ValueError(f"[ì—ëŸ¬] src_tensorì— vocab_size({vocab_size}) ì´ˆê³¼ ID í¬í•¨")
        if src_tensor.dtype != torch.long:
            raise ValueError("[ì—ëŸ¬] src_tensorëŠ” long íƒ€ì…ì´ì–´ì•¼ í•©ë‹ˆë‹¤")

        # GPU ì´ë™
        if torch.cuda.is_available():
            src_tensor = src_tensor.cuda()
            src_len = src_len.cuda()

        batch = {
            "src": src_tensor,
            "srclen": src_len,
            "src_map": None
        }

        print("âœ… batch ìƒì„± ì™„ë£Œ:", {k: v.shape if isinstance(v, torch.Tensor) else v for k, v in batch.items()})

        # ë²ˆì—­ ì‹¤í–‰
        try:
            scores, predictions = self.translator.translate_batch(batch, attn_debug=False)
        except Exception as e:
            print(f"[ì—ëŸ¬] translate_batch ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            print(f"src_tensor.shape: {src_tensor.shape}")
            print(f"src_len.shape: {src_len.shape}")
            print(f"src_tensor ê°’ ë²”ìœ„: {src_tensor.min().item()} ~ {src_tensor.max().item()}")
            raise

        # ê²°ê³¼ ì²˜ë¦¬
        if predictions and len(predictions) > 0:
            prediction_ids = predictions[0][0]  # ì²« ë¬¸ì¥ì˜ ì²« beam ê²°ê³¼

            # IDë¥¼ í† í°ìœ¼ë¡œ ë³€í™˜
            predicted_tokens = []
            for idx in prediction_ids:
                if isinstance(idx, torch.Tensor):
                    idx = idx.item()

                # íŠ¹ìˆ˜ í† í° ì œì™¸
                if idx not in [self.sp.piece_to_id('<pad>'), 
                            self.sp.piece_to_id('</s>'), 
                            self.sp.piece_to_id('<s>'),
                            self.sp.unk_id()]:
                    token = self.sp.id_to_piece(idx)
                    predicted_tokens.append(token)

            # ë””ì½”ë”© (token -> ë¬¸ìì—´)
            english_text = self.sp.decode_pieces(predicted_tokens)
            return english_text

        return ""